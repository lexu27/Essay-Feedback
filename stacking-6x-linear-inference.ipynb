{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db728e7",
   "metadata": {
    "id": "fFN1KR2XFZI8",
    "papermill": {
     "duration": 0.011717,
     "end_time": "2022-11-28T23:25:43.566716",
     "exception": false,
     "start_time": "2022-11-28T23:25:43.554999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Library Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af82ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:25:43.590488Z",
     "iopub.status.busy": "2022-11-28T23:25:43.590065Z",
     "iopub.status.idle": "2022-11-28T23:25:51.173068Z",
     "shell.execute_reply": "2022-11-28T23:25:51.171893Z"
    },
    "papermill": {
     "duration": 7.598642,
     "end_time": "2022-11-28T23:25:51.175752",
     "exception": false,
     "start_time": "2022-11-28T23:25:43.577110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/transformers-422/transformers-4.24.0-py3-none-any.whl\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.20.1\r\n",
      "    Uninstalling transformers-4.20.1:\r\n",
      "      Successfully uninstalled transformers-4.20.1\r\n",
      "Successfully installed transformers-4.24.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall --no-deps --no-index /kaggle/input/transformers-422/transformers-4.24.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "191f715f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:25:51.198945Z",
     "iopub.status.busy": "2022-11-28T23:25:51.198630Z",
     "iopub.status.idle": "2022-11-28T23:25:59.723483Z",
     "shell.execute_reply": "2022-11-28T23:25:59.722347Z"
    },
    "id": "A4OW0qKyjzzY",
    "papermill": {
     "duration": 8.539177,
     "end_time": "2022-11-28T23:25:59.725930",
     "exception": false,
     "start_time": "2022-11-28T23:25:51.186753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pickle\n",
    "import transformers\n",
    "import os\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import glob\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "from text_unidecode import unidecode\n",
    "import torch\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "import re\n",
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import gc\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa7df71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:25:59.751286Z",
     "iopub.status.busy": "2022-11-28T23:25:59.749287Z",
     "iopub.status.idle": "2022-11-28T23:25:59.758033Z",
     "shell.execute_reply": "2022-11-28T23:25:59.757068Z"
    },
    "papermill": {
     "duration": 0.022759,
     "end_time": "2022-11-28T23:25:59.760027",
     "exception": false,
     "start_time": "2022-11-28T23:25:59.737268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.24.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4271847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:25:59.782822Z",
     "iopub.status.busy": "2022-11-28T23:25:59.782517Z",
     "iopub.status.idle": "2022-11-28T23:26:00.051208Z",
     "shell.execute_reply": "2022-11-28T23:26:00.050217Z"
    },
    "id": "lYX_09FZjzzc",
    "outputId": "6c51419c-5dd3-4b59-c768-1aeb98646b34",
    "papermill": {
     "duration": 0.283308,
     "end_time": "2022-11-28T23:26:00.054102",
     "exception": false,
     "start_time": "2022-11-28T23:25:59.770794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG1 = {\n",
    "    \"model_name\": \"../input/bigbirdrobertalarge/bigbird-roberta-large\",\n",
    "    \"type\": \"Other Models\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"../input/downloading-ell-bigbird\",\n",
    "    \"max_length\": 4096,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": None,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"dropout\": 0.0,\n",
    "    \"multisample\": False,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG1[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG1[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adadcc69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:00.077791Z",
     "iopub.status.busy": "2022-11-28T23:26:00.077468Z",
     "iopub.status.idle": "2022-11-28T23:26:01.116522Z",
     "shell.execute_reply": "2022-11-28T23:26:01.115268Z"
    },
    "papermill": {
     "duration": 1.053233,
     "end_time": "2022-11-28T23:26:01.118906",
     "exception": false,
     "start_time": "2022-11-28T23:26:00.065673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:447: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG2 = {\n",
    "    \"model_name\": \"../input/debertav3base\",\n",
    "    \"type\": \"Other Models\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"../input/downloading-ell-debertav3base-notebooks\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": None,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"dropout\": 0.0,\n",
    "    \"multisample\": False,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG2[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG2[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69bcf21c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:01.142803Z",
     "iopub.status.busy": "2022-11-28T23:26:01.142465Z",
     "iopub.status.idle": "2022-11-28T23:26:02.198266Z",
     "shell.execute_reply": "2022-11-28T23:26:02.196703Z"
    },
    "papermill": {
     "duration": 1.070705,
     "end_time": "2022-11-28T23:26:02.201155",
     "exception": false,
     "start_time": "2022-11-28T23:26:01.130450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG3 = {\n",
    "    \"model_name\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
    "    \"type\": \"Other Models\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"../input/debertav3large-ell-download\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": None,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"dropout\": 0.0,\n",
    "    \"multisample\": False,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG3[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG3[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "198c55ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:02.225571Z",
     "iopub.status.busy": "2022-11-28T23:26:02.225262Z",
     "iopub.status.idle": "2022-11-28T23:26:02.434440Z",
     "shell.execute_reply": "2022-11-28T23:26:02.433482Z"
    },
    "papermill": {
     "duration": 0.223818,
     "end_time": "2022-11-28T23:26:02.436808",
     "exception": false,
     "start_time": "2022-11-28T23:26:02.212990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG4 = {\n",
    "    \"model_name\": \"../input/allenailongformerbase4096/longformer\",\n",
    "    \"type\": \"Full Input\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"../input/downloading-ell-longformer\",\n",
    "    \"max_length\": 4096,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 32,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 8,\n",
    "    \"pooler\": None,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"dropout\": 0.0,\n",
    "    \"multisample\": False,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG4[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG4[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dcb1d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:02.464736Z",
     "iopub.status.busy": "2022-11-28T23:26:02.464422Z",
     "iopub.status.idle": "2022-11-28T23:26:03.438823Z",
     "shell.execute_reply": "2022-11-28T23:26:03.437859Z"
    },
    "papermill": {
     "duration": 0.990555,
     "end_time": "2022-11-28T23:26:03.441123",
     "exception": false,
     "start_time": "2022-11-28T23:26:02.450568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG5 = {\n",
    "    \"model_name\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
    "    \"type\": \"Other Models\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"../input/ell-pseudo-debertav3large-download\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": None,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"dropout\": 0.0,\n",
    "    \"multisample\": False,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG5[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG5[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c85f5e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:03.465174Z",
     "iopub.status.busy": "2022-11-28T23:26:03.464844Z",
     "iopub.status.idle": "2022-11-28T23:26:04.490841Z",
     "shell.execute_reply": "2022-11-28T23:26:04.489321Z"
    },
    "papermill": {
     "duration": 1.040453,
     "end_time": "2022-11-28T23:26:04.493009",
     "exception": false,
     "start_time": "2022-11-28T23:26:03.452556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG6 = {\n",
    "    \"model_name\": \"../input/debertav3base\",\n",
    "    \"type\": \"Attention Regression Head\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"../input/downloading-attention-regression-head-debertav3b\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": \"attention\",\n",
    "    \"layer_start\": 1,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"dropout\": 0.0,\n",
    "    \"multisample\": False,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG6[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG6[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9076b0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:04.517611Z",
     "iopub.status.busy": "2022-11-28T23:26:04.516788Z",
     "iopub.status.idle": "2022-11-28T23:26:05.487603Z",
     "shell.execute_reply": "2022-11-28T23:26:05.486586Z"
    },
    "papermill": {
     "duration": 0.985103,
     "end_time": "2022-11-28T23:26:05.489864",
     "exception": false,
     "start_time": "2022-11-28T23:26:04.504761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG7 = {\n",
    "    \"model_name\": \"../input/debertav3base\",\n",
    "    \"type\": \"Weighted Regression Head\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"../input/downloading-weighted-head-debertav3base\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": \"weighted\",\n",
    "    \"layer_start\": 9,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"dropout\": 0.0,\n",
    "    \"multisample\": False,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG7[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG7[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0f41da5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:05.515122Z",
     "iopub.status.busy": "2022-11-28T23:26:05.514782Z",
     "iopub.status.idle": "2022-11-28T23:26:06.485214Z",
     "shell.execute_reply": "2022-11-28T23:26:06.484194Z"
    },
    "papermill": {
     "duration": 0.985918,
     "end_time": "2022-11-28T23:26:06.487656",
     "exception": false,
     "start_time": "2022-11-28T23:26:05.501738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG8 = {\n",
    "    \"model_name\": \"../input/debertav3base\",\n",
    "    \"type\": \"Attention Regression Head + Multisample Dropout\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"../input/downloading-attention-multisample-debertav3base\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": \"attention\",\n",
    "    \"layer_start\": 1,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"dropout\": 0.3,\n",
    "    \"multisample\": True,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG8[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG8[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "507aabf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:06.512856Z",
     "iopub.status.busy": "2022-11-28T23:26:06.512547Z",
     "iopub.status.idle": "2022-11-28T23:26:07.496967Z",
     "shell.execute_reply": "2022-11-28T23:26:07.495492Z"
    },
    "papermill": {
     "duration": 1.000292,
     "end_time": "2022-11-28T23:26:07.499769",
     "exception": false,
     "start_time": "2022-11-28T23:26:06.499477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG9 = {\n",
    "    \"model_name\": \"../input/debertav3base\",\n",
    "    \"type\": \"Baseline L2 Loss\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"../input/downloading-debertav3b-l2\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": None,\n",
    "    \"layer_start\": 9,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"dropout\": 0.0,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"multisample\": False,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG9[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG9[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35708074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:07.529177Z",
     "iopub.status.busy": "2022-11-28T23:26:07.527345Z",
     "iopub.status.idle": "2022-11-28T23:26:08.503095Z",
     "shell.execute_reply": "2022-11-28T23:26:08.501867Z"
    },
    "papermill": {
     "duration": 0.992738,
     "end_time": "2022-11-28T23:26:08.505910",
     "exception": false,
     "start_time": "2022-11-28T23:26:07.513172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG10 = {\n",
    "    \"model_name\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
    "    \"type\": \"Smooth L1 Loss\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"/kaggle/input/downloading-debertav3large-smoothl1\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": None,\n",
    "    \"layer_start\": 9,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"dropout\": 0.0,\n",
    "    \"multisample\": False,\n",
    "    \"oof_path\": \"deberta-v3-large-L1.csv\", \n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG10[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG10[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fcacac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:08.530817Z",
     "iopub.status.busy": "2022-11-28T23:26:08.530196Z",
     "iopub.status.idle": "2022-11-28T23:26:09.724546Z",
     "shell.execute_reply": "2022-11-28T23:26:09.723503Z"
    },
    "papermill": {
     "duration": 1.209283,
     "end_time": "2022-11-28T23:26:09.726872",
     "exception": false,
     "start_time": "2022-11-28T23:26:08.517589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG11 = {\n",
    "    \"model_name\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
    "    \"type\": \"L2 Loss\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"/kaggle/input/downloading-debertav3large-l2\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": None,\n",
    "    \"layer_start\": 9,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"dropout\": 0.0,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"multisample\": False,\n",
    "    \"oof_path\": \"deberta-v3-large-L2.csv\", \n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG11[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG11[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df4b2025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:09.753201Z",
     "iopub.status.busy": "2022-11-28T23:26:09.751449Z",
     "iopub.status.idle": "2022-11-28T23:26:09.834865Z",
     "shell.execute_reply": "2022-11-28T23:26:09.833905Z"
    },
    "papermill": {
     "duration": 0.098627,
     "end_time": "2022-11-28T23:26:09.837355",
     "exception": false,
     "start_time": "2022-11-28T23:26:09.738728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG12 = {\n",
    "    \"model_name\": \"/kaggle/input/transformers/xlnet-large-cased\",\n",
    "    \"type\": \"Other Models\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"/kaggle/input/downloading-xlnet-large-l1\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": None,\n",
    "    \"dropout\": 0.0,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"multisample\": False,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG12[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG12[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0cb59df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:09.862188Z",
     "iopub.status.busy": "2022-11-28T23:26:09.861863Z",
     "iopub.status.idle": "2022-11-28T23:26:09.918181Z",
     "shell.execute_reply": "2022-11-28T23:26:09.917313Z"
    },
    "papermill": {
     "duration": 0.070873,
     "end_time": "2022-11-28T23:26:09.920138",
     "exception": false,
     "start_time": "2022-11-28T23:26:09.849265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG13 = {\n",
    "    \"model_name\": \"/kaggle/input/ernie20largeen/nghuyong/ernie-2.0-large-en\",\n",
    "    \"type\": \"Other Models\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"/kaggle/input/downloading-ernie-2-0-large\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": None,\n",
    "    \"dropout\": 0.0,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"multisample\": False,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG13[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG13[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10862ee3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:09.945118Z",
     "iopub.status.busy": "2022-11-28T23:26:09.944291Z",
     "iopub.status.idle": "2022-11-28T23:26:10.907020Z",
     "shell.execute_reply": "2022-11-28T23:26:10.906024Z"
    },
    "papermill": {
     "duration": 0.977457,
     "end_time": "2022-11-28T23:26:10.909386",
     "exception": false,
     "start_time": "2022-11-28T23:26:09.931929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG14 = {\n",
    "    \"model_name\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
    "    \"type\": \"DS Last\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"/kaggle/input/downloading-debertav3large-l1-ds-last\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 4,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 4,\n",
    "    \"pooler\": \"DS Last\",\n",
    "    \"layer_start\": 9,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"aux_weight\": 0.5,\n",
    "    \"dropout\": 0.1,\n",
    "    \"multisample\": False,\n",
    "    \"oof_path\": \"deberta-v3-large-L1-DS-Last.csv\", \n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG14[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG14[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4059bff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:10.935671Z",
     "iopub.status.busy": "2022-11-28T23:26:10.935351Z",
     "iopub.status.idle": "2022-11-28T23:26:11.896189Z",
     "shell.execute_reply": "2022-11-28T23:26:11.895177Z"
    },
    "papermill": {
     "duration": 0.976797,
     "end_time": "2022-11-28T23:26:11.898461",
     "exception": false,
     "start_time": "2022-11-28T23:26:10.921664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG15 = {\n",
    "    \"model_name\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
    "    \"type\": \"DS All\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"/kaggle/input/downloading-debertav3large-l1-ds-all\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 4,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 4,\n",
    "    \"pooler\": \"DS All\",\n",
    "    \"layer_start\": 9,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"aux_weight\": 0.5,\n",
    "    \"dropout\": 0.1,\n",
    "    \"multisample\": False,\n",
    "    \"oof_path\": \"deberta-v3-large-L1-DS-All.csv\", \n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG15[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG15[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "506eedba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:11.924513Z",
     "iopub.status.busy": "2022-11-28T23:26:11.923521Z",
     "iopub.status.idle": "2022-11-28T23:26:12.878423Z",
     "shell.execute_reply": "2022-11-28T23:26:12.877105Z"
    },
    "papermill": {
     "duration": 0.970553,
     "end_time": "2022-11-28T23:26:12.881158",
     "exception": false,
     "start_time": "2022-11-28T23:26:11.910605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG16 = {\n",
    "    \"model_name\": \"../input/deberta-v3-large/deberta-v3-large\",\n",
    "    \"type\": \"DS MaxPool\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"/kaggle/input/downloading-debertav3large-l1-maxpool\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 4,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 4,\n",
    "    \"pooler\": \"DS MaxPool\",\n",
    "    \"layer_start\": 9,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"aux_weight\": 0.5,\n",
    "    \"dropout\": 0.1,\n",
    "    \"multisample\": False,\n",
    "    \"oof_path\": \"deberta-v3-large-DS-MaxPool.csv\", \n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG16[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG16[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9671c1c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:12.907250Z",
     "iopub.status.busy": "2022-11-28T23:26:12.906922Z",
     "iopub.status.idle": "2022-11-28T23:26:12.955733Z",
     "shell.execute_reply": "2022-11-28T23:26:12.954873Z"
    },
    "papermill": {
     "duration": 0.064016,
     "end_time": "2022-11-28T23:26:12.957783",
     "exception": false,
     "start_time": "2022-11-28T23:26:12.893767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG17 = {\n",
    "    \"model_name\": \"/kaggle/input/transformers/funnel-transformer-large\",\n",
    "    \"type\": \"Smooth L1 Loss\",\n",
    "    \"targets\": ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n",
    "    \"weights\": \"/kaggle/input/downloading-funnel-l1\",\n",
    "    \"max_length\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"folds\": 4,\n",
    "    \"lr\": 2e-5, \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 6,\n",
    "    \"grad_accum\": 1,\n",
    "    \"pooler\": None,\n",
    "    \"layer_start\": 9,\n",
    "    \"weight_decay\": 0.3,\n",
    "    \"dropout\": 0.0,\n",
    "    \"grad_norm\": 1000,\n",
    "    \"multisample\": False,\n",
    "    \"oof_path\": \"funnel-L1.csv\", \n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}\n",
    "CFG17[\"tokenizer\"] = AutoTokenizer.from_pretrained(CFG17[\"model_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1721ab",
   "metadata": {
    "id": "ekR23mjnezO7",
    "papermill": {
     "duration": 0.011626,
     "end_time": "2022-11-28T23:26:12.981394",
     "exception": false,
     "start_time": "2022-11-28T23:26:12.969768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80cdb09f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:13.006627Z",
     "iopub.status.busy": "2022-11-28T23:26:13.005707Z",
     "iopub.status.idle": "2022-11-28T23:26:13.013289Z",
     "shell.execute_reply": "2022-11-28T23:26:13.012356Z"
    },
    "papermill": {
     "duration": 0.022277,
     "end_time": "2022-11-28T23:26:13.015393",
     "exception": false,
     "start_time": "2022-11-28T23:26:12.993116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WeightedLayerPooling(torch.nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None \\\n",
    "            else nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        return weighted_average\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "084fe12d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:13.039839Z",
     "iopub.status.busy": "2022-11-28T23:26:13.039568Z",
     "iopub.status.idle": "2022-11-28T23:26:13.046612Z",
     "shell.execute_reply": "2022-11-28T23:26:13.045711Z"
    },
    "papermill": {
     "duration": 0.021504,
     "end_time": "2022-11-28T23:26:13.048573",
     "exception": false,
     "start_time": "2022-11-28T23:26:13.027069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.W = nn.Linear(in_features, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "        self.out_features = hidden_dim\n",
    "\n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.W(features))\n",
    "        score = self.V(att)\n",
    "        attention_weights = torch.softmax(score, dim=0)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = torch.sum(context_vector, dim=0)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "429a9ae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:13.073845Z",
     "iopub.status.busy": "2022-11-28T23:26:13.073234Z",
     "iopub.status.idle": "2022-11-28T23:26:13.100285Z",
     "shell.execute_reply": "2022-11-28T23:26:13.099453Z"
    },
    "id": "EIj0e07d8crC",
    "papermill": {
     "duration": 0.041892,
     "end_time": "2022-11-28T23:26:13.102210",
     "exception": false,
     "start_time": "2022-11-28T23:26:13.060318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#OBVIOUSLY, CHANGE THIS AS YOU NEED. USE SELF.LOG FOR ALL IMPORTANT METRICS\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config, vocab_length, data_loader_len):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        self.vocab_length = vocab_length\n",
    "        self.base_model = AutoModel.from_pretrained(self.config['model_name'], output_hidden_states = True)  \n",
    "        self.base_model.resize_token_embeddings(vocab_length)\n",
    "        self.dropout = torch.nn.Dropout(p=CFG[\"dropout\"])\n",
    "        self.order = torch.LongTensor([5, 0, 1, 2, 3, 4]).cuda()\n",
    "        \n",
    "        if self.config[\"pooler\"] == \"weighted\":\n",
    "            self.pooler = WeightedLayerPooling(self.base_model.config.num_hidden_layers, layer_start = self.config[\"layer_start\"])  \n",
    "            self._init_weights(self.pooler.layer_weights)\n",
    "            \n",
    "        elif self.config[\"pooler\"] == \"attention\":\n",
    "            self.pooler = AttentionPooling(self.base_model.config.hidden_size, config[\"hidden_dim\"])\n",
    "\n",
    "        if self.config[\"multisample\"]:\n",
    "            self.dropout1 = nn.Dropout(0.1)\n",
    "            self.dropout2 = nn.Dropout(0.2)\n",
    "            self.dropout3 = nn.Dropout(0.3)\n",
    "            self.dropout4 = nn.Dropout(0.4)\n",
    "            self.dropout5 = nn.Dropout(0.5)\n",
    "            \n",
    "        self.dropout = nn.Dropout(self.config[\"dropout\"])\n",
    "        self.fc = nn.Linear(self.base_model.config.hidden_size, 6)\n",
    "        self._init_weights(self.fc)\n",
    "        \n",
    "        if config[\"pooler\"] == \"DS All\" or config[\"pooler\"] == \"DS MaxPool\":\n",
    "            self.fcs = nn.ModuleList([])\n",
    "            for _ in range(6):\n",
    "                layer = nn.Linear(self.base_model.config.hidden_size, 1)\n",
    "                self._init_weights(layer)\n",
    "                self.fcs.append(layer)\n",
    "        else:\n",
    "            self.fcs = nn.ModuleList([])\n",
    "            for _ in range(7):\n",
    "                layer = nn.Linear(self.base_model.config.hidden_size, 1)\n",
    "                self._init_weights(layer)\n",
    "                self.fcs.append(layer)\n",
    "        \n",
    "        self.data_loader_len = data_loader_len\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.base_model.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def feature(self, inputs):\n",
    "\n",
    "        if self.config[\"pooler\"] == \"weighted\":\n",
    "            input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "        \n",
    "            x = self.base_model(input_ids = input_ids, attention_mask = attention_mask)[\"hidden_states\"]\n",
    "\n",
    "            x = torch.stack(x)\n",
    "            cls_embeddings = self.pooler(x)[:, 0]\n",
    "\n",
    "            return cls_embeddings\n",
    "        \n",
    "        \n",
    "        elif self.config[\"pooler\"] == \"DS Last\":\n",
    "            input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "        \n",
    "            x = self.base_model(input_ids = input_ids, attention_mask = attention_mask)[\"hidden_states\"]\n",
    "\n",
    "            x = torch.stack(x)\n",
    "\n",
    "            # Last 6 layers\n",
    "            return x[-7:, :, :, :]\n",
    "        \n",
    "        elif self.config[\"pooler\"] == \"DS All\":\n",
    "            input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "        \n",
    "            x = self.base_model(input_ids = input_ids, attention_mask = attention_mask)[\"hidden_states\"]\n",
    "\n",
    "            x = torch.stack(x)\n",
    "\n",
    "            # Last 6 layers\n",
    "            return x[-6:, :, :, :]\n",
    "        \n",
    "        elif self.config[\"pooler\"] == \"DS MaxPool\":\n",
    "            input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "        \n",
    "            x = self.base_model(input_ids = input_ids, attention_mask = attention_mask)[\"hidden_states\"]\n",
    "\n",
    "            x = torch.stack(x)\n",
    "\n",
    "            # Last 6 layers\n",
    "            return x[-6:, :, :, :]\n",
    "            \n",
    "        else:\n",
    "            input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "        \n",
    "            x = self.base_model(input_ids = input_ids, attention_mask = attention_mask)[\"last_hidden_state\"]\n",
    "\n",
    "            return x[:, 0, :]\n",
    "\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        features = self.feature(inputs)\n",
    "        \n",
    "        if self.config[\"multisample\"]:\n",
    "            logits1 = self.fc(self.dropout1(features))\n",
    "            logits2 = self.fc(self.dropout2(features))\n",
    "            logits3 = self.fc(self.dropout3(features))\n",
    "            logits4 = self.fc(self.dropout4(features))\n",
    "            logits5 = self.fc(self.dropout5(features))\n",
    "\n",
    "            logits = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
    "            \n",
    "            return logits\n",
    "\n",
    "            \n",
    "        if self.config[\"pooler\"] == \"DS Last\":\n",
    "\n",
    "            outputs = []\n",
    "\n",
    "            for layer_num, layer in enumerate(features):\n",
    "\n",
    "                if layer_num == (len(features) - 1):\n",
    "                    pred = self.fc(self.dropout(layer[:, 0, :]))\n",
    "                    break\n",
    "            \n",
    "                outputs.append(self.fcs[layer_num](self.dropout(layer[:, 0, :])))\n",
    "\n",
    "            outputs = torch.stack(outputs)\n",
    "            \n",
    "            return pred\n",
    "        \n",
    "        elif self.config[\"pooler\"] == \"DS MaxPool\":\n",
    "\n",
    "            outputs = []\n",
    "\n",
    "            layers = []\n",
    "\n",
    "            for layer_num, layer in enumerate(features):\n",
    "                \n",
    "                layers.append(self.dropout(layer[:, 0, :]))\n",
    "                \n",
    "                outputs.append(self.fcs[layer_num](layers[-1]))\n",
    "\n",
    "            outputs = torch.stack(outputs)\n",
    "\n",
    "            layers = torch.stack(layers)\n",
    "\n",
    "            final_cls = torch.max(layers, dim = 0)[0]\n",
    "\n",
    "            pred = self.fc(final_cls)\n",
    "            \n",
    "            return pred\n",
    "        \n",
    "        elif self.config[\"pooler\"] == \"DS All\":\n",
    "\n",
    "            outputs = []\n",
    "\n",
    "            layers = []\n",
    "\n",
    "            for layer_num, layer in enumerate(features):\n",
    "                \n",
    "                layers.append(self.dropout(layer[:, 0, :]))\n",
    "                \n",
    "                outputs.append(self.fcs[layer_num](layers[-1]))\n",
    "            \n",
    "            outputs = torch.stack(outputs)\n",
    "\n",
    "            return torch.index_select(outputs.squeeze(-1).transpose(0,1), 1, self.order)\n",
    "        \n",
    "        else:\n",
    "            logits = self.fc(features)\n",
    "            \n",
    "            return logits\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ab41d",
   "metadata": {
    "id": "kPPDgVLsjzzd",
    "papermill": {
     "duration": 0.011503,
     "end_time": "2022-11-28T23:26:13.125342",
     "exception": false,
     "start_time": "2022-11-28T23:26:13.113839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91ec297a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:13.150324Z",
     "iopub.status.busy": "2022-11-28T23:26:13.150019Z",
     "iopub.status.idle": "2022-11-28T23:26:13.161336Z",
     "shell.execute_reply": "2022-11-28T23:26:13.160439Z"
    },
    "id": "sTadJIpv1LGs",
    "papermill": {
     "duration": 0.026218,
     "end_time": "2022-11-28T23:26:13.163330",
     "exception": false,
     "start_time": "2022-11-28T23:26:13.137112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TestData(Dataset):\n",
    "    def __init__(self, df, config, special_tokens = None):\n",
    "        self.df = df\n",
    "        self.esc_chars = ['\\\"', \"\\\\\", \"\\n\", \"\\r\", \"\\t\", \"\\b\", \"\\f\", \"\\v\", \":)\", \";)\", \":(\", \"uwu\", \"owo\", \"xd\", \":3\", \":-)\", \":D\", \">:(\", \"\\xa0\", \"\\x92\", \"\\x93\", \"\\x91\", \"\\x94\", \"\\x97\", \"x\\B4\", \"\\x96\", \"\\x82\", \"\\x84\"]\n",
    "        self.df[\"full_text\"] = self.df[\"full_text\"].apply(lambda text: self.resolve_encodings_and_normalize(text))\n",
    "\n",
    "        codecs.register_error(\"replace_encoding_with_utf8\", self.replace_encoding_with_utf8)\n",
    "        codecs.register_error(\"replace_decoding_with_cp1252\", self.replace_decoding_with_cp1252)\n",
    "\n",
    "    def replace_encoding_with_utf8(self, error):\n",
    "        return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "\n",
    "    def replace_decoding_with_cp1252(self, error):\n",
    "        return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "\n",
    "    def resolve_encodings_and_normalize(self, text: str) -> str:\n",
    "        text = (\n",
    "            text.encode(\"raw_unicode_escape\")\n",
    "            .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "            .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "            .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        )\n",
    "        \n",
    "        text = unidecode(text)\n",
    "        \n",
    "        return self.remove_esc_chars(text)\n",
    "\n",
    "    def remove_esc_chars(self, text):\n",
    "        txt = deepcopy(text)\n",
    "        for char in self.esc_chars:\n",
    "            if char == '\\\"':\n",
    "                txt = txt.replace(char, '\"')\n",
    "            elif char == \"\\x92\" or char == \"\\x91\" or char == \"\\xB4\":\n",
    "                txt = txt.replace(char, \"'\")\n",
    "            elif char == \"\\0x93\" or char == \"\\0x94\":\n",
    "                txt = txt.replace(char, '\"')\n",
    "            elif char == \"\\0x97\" or char == \"\\0x96\":\n",
    "                txt = txt.replace(char, '-')\n",
    "            elif char == \"\\0x82\" or char == \"\\0x84\":\n",
    "                txt = txt.replace(char, ',')\n",
    "            else:\n",
    "                txt = txt.replace(char, ' ')\n",
    "        return txt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.df.iloc[idx][\"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9526ebfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:13.188165Z",
     "iopub.status.busy": "2022-11-28T23:26:13.187386Z",
     "iopub.status.idle": "2022-11-28T23:26:13.192767Z",
     "shell.execute_reply": "2022-11-28T23:26:13.191944Z"
    },
    "id": "WwuADIRbwAqs",
    "papermill": {
     "duration": 0.019755,
     "end_time": "2022-11-28T23:26:13.194690",
     "exception": false,
     "start_time": "2022-11-28T23:26:13.174935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def construct_collate_fn(config):\n",
    "    def collate_dynamic_padding(batch):\n",
    "        # Dynamic Padding tokenization\n",
    "        sentences = config[\"tokenizer\"](batch, padding=True, max_length = config[\"max_length\"], truncation = True, return_token_type_ids = False, return_tensors=\"pt\")\n",
    "        return sentences\n",
    "    \n",
    "    return collate_dynamic_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f132f194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:13.219801Z",
     "iopub.status.busy": "2022-11-28T23:26:13.218945Z",
     "iopub.status.idle": "2022-11-28T23:26:13.224687Z",
     "shell.execute_reply": "2022-11-28T23:26:13.223851Z"
    },
    "id": "4lREk9P6jzzg",
    "papermill": {
     "duration": 0.020296,
     "end_time": "2022-11-28T23:26:13.226630",
     "exception": false,
     "start_time": "2022-11-28T23:26:13.206334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CHANGE AS NEEDED. MOST OF THE TIME, PYTORCH'S DEFAULT COLLATOR IS ENOUGH.\n",
    "class DataModule():\n",
    "    def __init__(self, config, test, collate_fn):\n",
    "        self.config = config\n",
    "        self.test = test\n",
    "        self.collate_fn = collate_fn\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_loader = DataLoader(self.test, batch_size = self.config[\"batch_size\"], collate_fn = self.collate_fn)      \n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bdf99b",
   "metadata": {
    "id": "L8HsBZmivxSj",
    "papermill": {
     "duration": 0.011514,
     "end_time": "2022-11-28T23:26:13.249740",
     "exception": false,
     "start_time": "2022-11-28T23:26:13.238226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "653bc9f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:13.274831Z",
     "iopub.status.busy": "2022-11-28T23:26:13.274073Z",
     "iopub.status.idle": "2022-11-28T23:26:13.280144Z",
     "shell.execute_reply": "2022-11-28T23:26:13.279324Z"
    },
    "papermill": {
     "duration": 0.020593,
     "end_time": "2022-11-28T23:26:13.282084",
     "exception": false,
     "start_time": "2022-11-28T23:26:13.261491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, loader):\n",
    "    outputs = []\n",
    "    device = torch.device('cuda')\n",
    "    model = model.to(device).eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(loader):\n",
    "            for key, value in inputs.items():\n",
    "                inputs[key] = value.to(device)\n",
    "            predictions = model(inputs)\n",
    "            outputs.append(predictions)\n",
    "    return torch.cat(tuple(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7044547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:13.306975Z",
     "iopub.status.busy": "2022-11-28T23:26:13.306250Z",
     "iopub.status.idle": "2022-11-28T23:26:13.313631Z",
     "shell.execute_reply": "2022-11-28T23:26:13.312814Z"
    },
    "papermill": {
     "duration": 0.021786,
     "end_time": "2022-11-28T23:26:13.315552",
     "exception": false,
     "start_time": "2022-11-28T23:26:13.293766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(loader, num_preds, path, config, save_path):\n",
    "    predictions = []\n",
    "    for fold in range(num_preds):\n",
    "        \n",
    "        model = Model(config, len(config[\"tokenizer\"]), len(loader))\n",
    "        checkpoint = torch.load(f\"{path}/fold-{fold}.pt\", map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'], strict = False)               \n",
    "        \n",
    "        results = predict(model, loader)\n",
    "        \n",
    "        predictions.append(results.cpu().numpy())\n",
    "        \n",
    "        del model, checkpoint; gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    predictions = np.mean(np.array(predictions), axis = 0)\n",
    "    df = pd.DataFrame(predictions, columns = CFG[\"targets\"])\n",
    "    df.to_csv(save_path)\n",
    "    \n",
    "    del predictions, df; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bf85aba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:13.340808Z",
     "iopub.status.busy": "2022-11-28T23:26:13.340069Z",
     "iopub.status.idle": "2022-11-28T23:26:13.345003Z",
     "shell.execute_reply": "2022-11-28T23:26:13.344214Z"
    },
    "papermill": {
     "duration": 0.019653,
     "end_time": "2022-11-28T23:26:13.346937",
     "exception": false,
     "start_time": "2022-11-28T23:26:13.327284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFGS = [CFG1, CFG2, CFG3, CFG4, CFG5, CFG6, CFG7, CFG8, CFG9, CFG10, CFG11, CFG12, CFG13, CFG14, CFG15, CFG16, CFG17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aec1cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T23:26:13.372077Z",
     "iopub.status.busy": "2022-11-28T23:26:13.371315Z",
     "iopub.status.idle": "2022-11-29T00:17:05.669887Z",
     "shell.execute_reply": "2022-11-29T00:17:05.667916Z"
    },
    "papermill": {
     "duration": 3052.315759,
     "end_time": "2022-11-29T00:17:05.674363",
     "exception": false,
     "start_time": "2022-11-28T23:26:13.358604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/bigbirdrobertalarge/bigbird-roberta-large were not used when initializing BigBirdModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/big_bird/modeling_big_bird.py:979: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  * num_indices_to_pick_from\n",
      "100%|| 1/1 [00:01<00:00,  1.32s/it]\n",
      "Some weights of the model checkpoint at ../input/bigbirdrobertalarge/bigbird-roberta-large were not used when initializing BigBirdModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  2.02it/s]\n",
      "Some weights of the model checkpoint at ../input/bigbirdrobertalarge/bigbird-roberta-large were not used when initializing BigBirdModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  2.07it/s]\n",
      "Some weights of the model checkpoint at ../input/bigbirdrobertalarge/bigbird-roberta-large were not used when initializing BigBirdModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BigBirdModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  2.02it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  4.98it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.16it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.68it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.55it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.46it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.35it/s]\n",
      "100%|| 1/1 [00:00<00:00, 16.61it/s]\n",
      "100%|| 1/1 [00:00<00:00, 23.55it/s]\n",
      "100%|| 1/1 [00:00<00:00, 30.16it/s]\n",
      "100%|| 1/1 [00:00<00:00, 22.93it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.21it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.65it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.80it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.68it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.23it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.67it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.11it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00, 10.25it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.45it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  8.56it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00, 10.33it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.70it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.35it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.47it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.62it/s]\n",
      "Some weights of the model checkpoint at ../input/debertav3base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.72it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.22it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.47it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.40it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.40it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.49it/s]\n",
      "Some weights of the model checkpoint at /kaggle/input/transformers/xlnet-large-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  9.62it/s]\n",
      "Some weights of the model checkpoint at /kaggle/input/transformers/xlnet-large-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00, 15.65it/s]\n",
      "Some weights of the model checkpoint at /kaggle/input/transformers/xlnet-large-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00, 18.61it/s]\n",
      "Some weights of the model checkpoint at /kaggle/input/transformers/xlnet-large-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00, 19.36it/s]\n",
      "100%|| 1/1 [00:00<00:00, 19.34it/s]\n",
      "100%|| 1/1 [00:00<00:00, 24.73it/s]\n",
      "100%|| 1/1 [00:00<00:00, 19.15it/s]\n",
      "100%|| 1/1 [00:00<00:00, 25.16it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.10it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.39it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.57it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.18it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.51it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.53it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.23it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.42it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.39it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|| 1/1 [00:00<00:00,  3.40it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n",
      "100%|| 1/1 [00:00<00:00,  4.52it/s]\n",
      "100%|| 1/1 [00:00<00:00,  5.74it/s]\n",
      "100%|| 1/1 [00:00<00:00,  5.50it/s]\n",
      "100%|| 1/1 [00:00<00:00,  6.22it/s]\n"
     ]
    }
   ],
   "source": [
    "models = ['bigbird-roberta-large', 'deberta-v3-baseline', 'deberta-v3-large', 'longformer-base', 'deberta-v3-large-psuedo', 'deberta-v3-base-attention-head', 'deberta-v3-base-weighted-head', 'deberta-v3-base-attention-multisample', 'deberta-v3-baseline-L2', \"deberta-v3-large-L1\", \"deberta-v3-large-L2\", \"xlnet-large-cased-L1\", \"ernie-2\", \"deberta-v3-large-L1-Last\", \"deberta-v3-large-L1-DS-All\", \"deberta-v3-large-DS-MaxPool\", \"funnel-L1\"]\n",
    "for num, CFG in enumerate(CFGS):\n",
    "    \n",
    "    data = pd.read_csv(f\"../input/feedback-prize-english-language-learning/test.csv\")\n",
    "\n",
    "    test = TestData(data, CFG)\n",
    "    dataset = DataModule(CFG, test, construct_collate_fn(CFG))\n",
    "    loader = dataset.test_dataloader()\n",
    "    \n",
    "    oof_name = f\"{models[num]}.csv\"\n",
    "\n",
    "    get_predictions(loader, num_preds = 4, path=CFG[\"weights\"], config = CFG, save_path = oof_name)\n",
    "    \n",
    "    del data, test, dataset, loader; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e007c",
   "metadata": {
    "papermill": {
     "duration": 0.028887,
     "end_time": "2022-11-29T00:17:05.731745",
     "exception": false,
     "start_time": "2022-11-29T00:17:05.702858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88daa1c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:05.790568Z",
     "iopub.status.busy": "2022-11-29T00:17:05.790229Z",
     "iopub.status.idle": "2022-11-29T00:17:05.797735Z",
     "shell.execute_reply": "2022-11-29T00:17:05.796726Z"
    },
    "papermill": {
     "duration": 0.039446,
     "end_time": "2022-11-29T00:17:05.799766",
     "exception": false,
     "start_time": "2022-11-29T00:17:05.760320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    \"model_name\": \"6x Linear\",\n",
    "    \"models\": [\n",
    "            \"deberta-v3-large-L1\",\n",
    "            \"deberta-v3-baseline-L2\",\n",
    "            \"deberta-v3-large\",\n",
    "            \"longformer-base\",\n",
    "            \"deberta-v3-large-psuedo\",\n",
    "            \"deberta-v3-baseline\",\n",
    "            \"deberta-v3-base-attention-head\",\n",
    "            \"deberta-v3-base-attention-multisample\",\n",
    "            \"bigbird-roberta-large\",\n",
    "            \"deberta-v3-base-weighted-head\",\n",
    "            \"ernie-2\",\n",
    "            \"funnel-L1\",\n",
    "            \"xlnet-large-cased-L1\",\n",
    "            \"deberta-v3-large-DS-MaxPool\",\n",
    "            \"deberta-v3-large-L1-Last\",\n",
    "            \"deberta-v3-large-L1-DS-All\",\n",
    "            \"deberta-v3-large-L2\"\n",
    "        ],\n",
    "    \"type\": \"Stacking\",\n",
    "    \"seed\": 42,\n",
    "    \"lr\": 1e-3,\n",
    "    \"final_div_factor\": 1e8,\n",
    "    \"batch_size\": 4,\n",
    "    \"epochs\": 20,\n",
    "    \"num_warmup_steps\": 0.0,\n",
    "    \"patience\": 5,\n",
    "    \"grad_accum\": 1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \n",
    "    \"optimizer\": \"one_cycle\",\n",
    "    \"scheduler\": \"linear\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5833de0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:05.856139Z",
     "iopub.status.busy": "2022-11-29T00:17:05.855792Z",
     "iopub.status.idle": "2022-11-29T00:17:05.862093Z",
     "shell.execute_reply": "2022-11-29T00:17:05.861065Z"
    },
    "papermill": {
     "duration": 0.037419,
     "end_time": "2022-11-29T00:17:05.864631",
     "exception": false,
     "start_time": "2022-11-29T00:17:05.827212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#OBVIOUSLY, CHANGE THIS AS YOU NEED. USE SELF.LOG FOR ALL IMPORTANT METRICS\n",
    "class Stacker(nn.Module):\n",
    "    def __init__(self, config, data_loader_len):\n",
    "        super(Stacker, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.fc = nn.Linear(len(self.config[\"models\"]), 1)\n",
    "\n",
    "        self.data_loader_len = data_loader_len\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        return self.fc(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c614fe4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:05.920867Z",
     "iopub.status.busy": "2022-11-29T00:17:05.920589Z",
     "iopub.status.idle": "2022-11-29T00:17:05.926291Z",
     "shell.execute_reply": "2022-11-29T00:17:05.925395Z"
    },
    "papermill": {
     "duration": 0.036146,
     "end_time": "2022-11-29T00:17:05.928319",
     "exception": false,
     "start_time": "2022-11-29T00:17:05.892173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, base_model_dfs, config):\n",
    "        self.base_model_dfs = base_model_dfs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.base_model_dfs[\"deberta-v3-large\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        predictions = []\n",
    "        for model in self.base_model_dfs.values():\n",
    "            predictions.append(model.iloc[idx][1:].values.astype(float))\n",
    "        \n",
    "        return np.array(predictions).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd8ab2f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:05.984402Z",
     "iopub.status.busy": "2022-11-29T00:17:05.984131Z",
     "iopub.status.idle": "2022-11-29T00:17:05.988424Z",
     "shell.execute_reply": "2022-11-29T00:17:05.987465Z"
    },
    "papermill": {
     "duration": 0.034653,
     "end_time": "2022-11-29T00:17:05.990324",
     "exception": false,
     "start_time": "2022-11-29T00:17:05.955671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_dynamic_padding(batch):\n",
    "    \n",
    "    predictions = torch.tensor(batch)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39b72109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:06.046943Z",
     "iopub.status.busy": "2022-11-29T00:17:06.046662Z",
     "iopub.status.idle": "2022-11-29T00:17:06.109563Z",
     "shell.execute_reply": "2022-11-29T00:17:06.108733Z"
    },
    "papermill": {
     "duration": 0.093374,
     "end_time": "2022-11-29T00:17:06.111694",
     "exception": false,
     "start_time": "2022-11-29T00:17:06.018320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_id = pd.read_csv(\"../input/feedback-prize-english-language-learning/test.csv\").drop(\"full_text\", axis = 1)\n",
    "\n",
    "base_model_dfs = {}\n",
    "for model in CFG[\"models\"]:\n",
    "    path = f\"./{model}.csv\"\n",
    "    base_model_dfs[model] = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc96aaa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:06.168345Z",
     "iopub.status.busy": "2022-11-29T00:17:06.168014Z",
     "iopub.status.idle": "2022-11-29T00:17:06.172865Z",
     "shell.execute_reply": "2022-11-29T00:17:06.172012Z"
    },
    "papermill": {
     "duration": 0.035183,
     "end_time": "2022-11-29T00:17:06.174778",
     "exception": false,
     "start_time": "2022-11-29T00:17:06.139595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = Data(base_model_dfs, CFG)\n",
    "dataset = DataModule(CFG, train, collate_dynamic_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "093b52b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:06.231144Z",
     "iopub.status.busy": "2022-11-29T00:17:06.230844Z",
     "iopub.status.idle": "2022-11-29T00:17:06.235140Z",
     "shell.execute_reply": "2022-11-29T00:17:06.234177Z"
    },
    "papermill": {
     "duration": 0.03529,
     "end_time": "2022-11-29T00:17:06.237430",
     "exception": false,
     "start_time": "2022-11-29T00:17:06.202140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = dataset.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e449ef4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:06.293453Z",
     "iopub.status.busy": "2022-11-29T00:17:06.293147Z",
     "iopub.status.idle": "2022-11-29T00:17:06.297974Z",
     "shell.execute_reply": "2022-11-29T00:17:06.296999Z"
    },
    "papermill": {
     "duration": 0.035145,
     "end_time": "2022-11-29T00:17:06.300032",
     "exception": false,
     "start_time": "2022-11-29T00:17:06.264887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacker = Stacker(CFG, len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef27b62f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:06.356176Z",
     "iopub.status.busy": "2022-11-29T00:17:06.355868Z",
     "iopub.status.idle": "2022-11-29T00:17:06.381240Z",
     "shell.execute_reply": "2022-11-29T00:17:06.380412Z"
    },
    "papermill": {
     "duration": 0.055922,
     "end_time": "2022-11-29T00:17:06.383335",
     "exception": false,
     "start_time": "2022-11-29T00:17:06.327413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"../input/downloading-stacking-linear6x/stacker-6x.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7055f05f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:06.439812Z",
     "iopub.status.busy": "2022-11-29T00:17:06.439532Z",
     "iopub.status.idle": "2022-11-29T00:17:06.446114Z",
     "shell.execute_reply": "2022-11-29T00:17:06.445106Z"
    },
    "papermill": {
     "duration": 0.037821,
     "end_time": "2022-11-29T00:17:06.448931",
     "exception": false,
     "start_time": "2022-11-29T00:17:06.411110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacker.load_state_dict(checkpoint['model_state_dict'], strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f23d9587",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:06.505372Z",
     "iopub.status.busy": "2022-11-29T00:17:06.505087Z",
     "iopub.status.idle": "2022-11-29T00:17:06.509402Z",
     "shell.execute_reply": "2022-11-29T00:17:06.508390Z"
    },
    "papermill": {
     "duration": 0.035051,
     "end_time": "2022-11-29T00:17:06.511709",
     "exception": false,
     "start_time": "2022-11-29T00:17:06.476658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3600d65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:06.599944Z",
     "iopub.status.busy": "2022-11-29T00:17:06.599529Z",
     "iopub.status.idle": "2022-11-29T00:17:06.609147Z",
     "shell.execute_reply": "2022-11-29T00:17:06.608091Z"
    },
    "papermill": {
     "duration": 0.072893,
     "end_time": "2022-11-29T00:17:06.612192",
     "exception": false,
     "start_time": "2022-11-29T00:17:06.539299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacker = stacker.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d17d5e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:06.719699Z",
     "iopub.status.busy": "2022-11-29T00:17:06.719178Z",
     "iopub.status.idle": "2022-11-29T00:17:06.750701Z",
     "shell.execute_reply": "2022-11-29T00:17:06.749116Z"
    },
    "papermill": {
     "duration": 0.079659,
     "end_time": "2022-11-29T00:17:06.752869",
     "exception": false,
     "start_time": "2022-11-29T00:17:06.673210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:207.)\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "final_predictions = []\n",
    "for batch in test_loader:\n",
    "    with torch.no_grad():\n",
    "        inputs = batch\n",
    "\n",
    "        inputs = inputs.to(device).to(torch.float32)\n",
    "        \n",
    "        y_hat = stacker(inputs).squeeze()\n",
    "    \n",
    "        final_predictions.append(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5409d2e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:06.836782Z",
     "iopub.status.busy": "2022-11-29T00:17:06.836477Z",
     "iopub.status.idle": "2022-11-29T00:17:06.885317Z",
     "shell.execute_reply": "2022-11-29T00:17:06.884438Z"
    },
    "papermill": {
     "duration": 0.093292,
     "end_time": "2022-11-29T00:17:06.887783",
     "exception": false,
     "start_time": "2022-11-29T00:17:06.794491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[2.8891, 2.7693, 3.0555, 3.0300, 2.7272, 2.7012],\n",
       "         [2.6773, 2.4880, 2.7181, 2.4632, 2.1987, 2.7133],\n",
       "         [3.5766, 3.4281, 3.5467, 3.5945, 3.4269, 3.4317]], device='cuda:0')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4729b5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:06.977902Z",
     "iopub.status.busy": "2022-11-29T00:17:06.977559Z",
     "iopub.status.idle": "2022-11-29T00:17:06.982207Z",
     "shell.execute_reply": "2022-11-29T00:17:06.981354Z"
    },
    "papermill": {
     "duration": 0.052957,
     "end_time": "2022-11-29T00:17:06.985985",
     "exception": false,
     "start_time": "2022-11-29T00:17:06.933028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_preds = torch.cat(final_predictions).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c95b37e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:07.072964Z",
     "iopub.status.busy": "2022-11-29T00:17:07.072584Z",
     "iopub.status.idle": "2022-11-29T00:17:07.083112Z",
     "shell.execute_reply": "2022-11-29T00:17:07.082141Z"
    },
    "papermill": {
     "duration": 0.068941,
     "end_time": "2022-11-29T00:17:07.095839",
     "exception": false,
     "start_time": "2022-11-29T00:17:07.026898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.8891196, 2.7692761, 3.055546 , 3.0299673, 2.7271621, 2.7011757],\n",
       "       [2.6773233, 2.487992 , 2.7180629, 2.4631908, 2.1986609, 2.71327  ],\n",
       "       [3.576643 , 3.4281452, 3.5466895, 3.594492 , 3.426944 , 3.4317079]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0468834a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:07.163164Z",
     "iopub.status.busy": "2022-11-29T00:17:07.162807Z",
     "iopub.status.idle": "2022-11-29T00:17:07.167464Z",
     "shell.execute_reply": "2022-11-29T00:17:07.166462Z"
    },
    "papermill": {
     "duration": 0.036162,
     "end_time": "2022-11-29T00:17:07.169588",
     "exception": false,
     "start_time": "2022-11-29T00:17:07.133426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(text_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bbb5203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:07.226359Z",
     "iopub.status.busy": "2022-11-29T00:17:07.226074Z",
     "iopub.status.idle": "2022-11-29T00:17:07.237546Z",
     "shell.execute_reply": "2022-11-29T00:17:07.236698Z"
    },
    "papermill": {
     "duration": 0.042399,
     "end_time": "2022-11-29T00:17:07.239599",
     "exception": false,
     "start_time": "2022-11-29T00:17:07.197200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission[\"cohesion\"], submission[\"syntax\"], submission[\"vocabulary\"], submission[\"phraseology\"], submission[\"grammar\"], submission[\"conventions\"] = submission_preds[:,0], submission_preds[:,1], submission_preds[:,2], submission_preds[:,3], submission_preds[:,4], submission_preds[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbbcd768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:07.296180Z",
     "iopub.status.busy": "2022-11-29T00:17:07.295878Z",
     "iopub.status.idle": "2022-11-29T00:17:07.313810Z",
     "shell.execute_reply": "2022-11-29T00:17:07.312858Z"
    },
    "papermill": {
     "duration": 0.048514,
     "end_time": "2022-11-29T00:17:07.315790",
     "exception": false,
     "start_time": "2022-11-29T00:17:07.267276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>2.889120</td>\n",
       "      <td>2.769276</td>\n",
       "      <td>3.055546</td>\n",
       "      <td>3.029967</td>\n",
       "      <td>2.727162</td>\n",
       "      <td>2.701176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>2.677323</td>\n",
       "      <td>2.487992</td>\n",
       "      <td>2.718063</td>\n",
       "      <td>2.463191</td>\n",
       "      <td>2.198661</td>\n",
       "      <td>2.713270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3.576643</td>\n",
       "      <td>3.428145</td>\n",
       "      <td>3.546690</td>\n",
       "      <td>3.594492</td>\n",
       "      <td>3.426944</td>\n",
       "      <td>3.431708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion    syntax  vocabulary  phraseology   grammar  \\\n",
       "0  0000C359D63E  2.889120  2.769276    3.055546     3.029967  2.727162   \n",
       "1  000BAD50D026  2.677323  2.487992    2.718063     2.463191  2.198661   \n",
       "2  00367BB2546B  3.576643  3.428145    3.546690     3.594492  3.426944   \n",
       "\n",
       "   conventions  \n",
       "0     2.701176  \n",
       "1     2.713270  \n",
       "2     3.431708  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29671b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T00:17:07.373711Z",
     "iopub.status.busy": "2022-11-29T00:17:07.372894Z",
     "iopub.status.idle": "2022-11-29T00:17:07.379470Z",
     "shell.execute_reply": "2022-11-29T00:17:07.378648Z"
    },
    "papermill": {
     "duration": 0.037563,
     "end_time": "2022-11-29T00:17:07.381390",
     "exception": false,
     "start_time": "2022-11-29T00:17:07.343827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb19c9",
   "metadata": {
    "papermill": {
     "duration": 0.027635,
     "end_time": "2022-11-29T00:17:07.436569",
     "exception": false,
     "start_time": "2022-11-29T00:17:07.408934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3094.614964,
   "end_time": "2022-11-29T00:17:10.733562",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-28T23:25:36.118598",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
